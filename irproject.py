# -*- coding: utf-8 -*-
"""IRproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nan3Z0TlLxRcJCzxcAGHIRUTX7JLwDVu
"""

import re
import math
import string
import nltk
from collections import defaultdict
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

from nltk.tokenize import word_tokenize
class MiniSearchEngine:
    def __init__(self):
        # Initialize NLTK with error handling
        self.initialize_nltk()

        # Text processing components
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))

        # Search engine data structures
        self.documents = {}
        self.inverted_index = defaultdict(dict)
        self.tf_idf_index = defaultdict(dict)
        self.idf_values = {}
        self.doc_lengths = {}

    def initialize_nltk(self):
        """Initialize NLTK with fallback options"""
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            try:
                nltk.download('punkt')
            except:
                print("Warning: Couldn't download Punkt tokenizer. Using simple tokenizer as fallback.")

        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords')

    def tokenize_text(self, text):
        """Tokenize text with fallback to simple tokenization"""
        try:
            return word_tokenize(text)
        except:
            # Fallback simple tokenizer
            return re.findall(r"\w+(?:'\w+)?|\S", text)

    def preprocess_text(self, text):
        """Tokenize and preprocess text"""
        # Tokenization with fallback
        tokens = self.tokenize_text(text)

        # Lowercasing
        tokens = [token.lower() for token in tokens]

        # Remove punctuation and numbers
        tokens = [token for token in tokens if token not in string.punctuation and not token.isdigit()]

        # Remove stopwords
        tokens = [token for token in tokens if token not in self.stop_words]

        # Stemming
        tokens = [self.stemmer.stem(token) for token in tokens]

        return tokens

    def parse_cranfield_documents(self, file_path):
        """Parse Cranfield documents file with robust error handling"""
        documents = {}
        current_id = None
        current_text = ""

        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    if line.startswith(".I"):
                        if current_id is not None:
                            documents[current_id] = current_text.strip()
                        current_id = line.split()[1]
                        current_text = ""
                    elif line.startswith(".W"):
                        continue
                    else:
                        current_text += line

                if current_id is not None:
                    documents[current_id] = current_text.strip()
        except FileNotFoundError:
            print(f"Error: File '{file_path}' not found.")
            return {}
        except Exception as e:
            print(f"Error reading file: {str(e)}")
            return {}

        return documents

    def build_inverted_index(self, documents):
        """Build inverted index with term frequencies"""
        inverted_index = defaultdict(dict)
        doc_lengths = {}

        for doc_id, text in documents.items():
            tokens = self.preprocess_text(text)
            doc_lengths[doc_id] = len(tokens)

            term_freq = defaultdict(int)
            for term in tokens:
                term_freq[term] += 1

            for term, freq in term_freq.items():
                inverted_index[term][doc_id] = freq

        return inverted_index, doc_lengths

    def compute_tf_idf(self, inverted_index, total_docs):
        """Compute TF-IDF weights"""
        tf_idf_index = defaultdict(dict)
        idf_values = {}

        for term, doc_freqs in inverted_index.items():
            doc_count = len(doc_freqs)
            idf = math.log(total_docs / (1 + doc_count))
            idf_values[term] = idf

            for doc_id, tf in doc_freqs.items():
                tf_idf = (1 + math.log(tf)) * idf
                tf_idf_index[term][doc_id] = tf_idf

        return tf_idf_index, idf_values

    def load_documents(self, file_path):
        """Load and index documents"""
        self.documents = self.parse_cranfield_documents(file_path)
        if not self.documents:
            print("No documents loaded. Check your file path.")
            return False

        self.inverted_index, self.doc_lengths = self.build_inverted_index(self.documents)
        self.tf_idf_index, self.idf_values = self.compute_tf_idf(self.inverted_index, len(self.documents))
        return True

    def search(self, query, top_k=10):
        """Search documents and return top results"""
        if not self.inverted_index:
            print("Error: No documents indexed. Please load documents first.")
            return []

        query_terms = self.preprocess_text(query)
        if not query_terms:
            print("Error: Query contains only stopwords or punctuation.")
            return []

        query_vector = defaultdict(float)
        query_term_freq = defaultdict(int)

        for term in query_terms:
            query_term_freq[term] += 1

        for term, freq in query_term_freq.items():
            if term in self.idf_values:
                tf = 1 + math.log(freq)
                query_vector[term] = tf * self.idf_values[term]

        doc_scores = defaultdict(float)
        query_norm = math.sqrt(sum(val**2 for val in query_vector.values()))

        relevant_docs = set()
        for term in query_terms:
            if term in self.tf_idf_index:
                relevant_docs.update(self.tf_idf_index[term].keys())

        for doc_id in relevant_docs:
            doc_vector = defaultdict(float)

            for term in query_vector:
                if term in self.tf_idf_index and doc_id in self.tf_idf_index[term]:
                    doc_vector[term] = self.tf_idf_index[term][doc_id]

            dot_product = sum(query_vector[term] * doc_vector.get(term, 0.0) for term in query_vector)
            doc_norm = math.sqrt(sum(val**2 for val in doc_vector.values()))

            if query_norm > 0 and doc_norm > 0:
                cosine_sim = dot_product / (query_norm * doc_norm)
                doc_scores[doc_id] = cosine_sim

        return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]

    def display_results(self, results):
        """Display search results"""
        if not results:
            print("No results found.")
            return

        print("\nSearch Results:")
        print("-" * 60)
        for rank, (doc_id, score) in enumerate(results, 1):
            print(f"{rank}. Document ID: {doc_id} | Score: {score:.4f}")
            print(f"   Preview: {self.documents.get(doc_id, '')[:100]}...\n")
        print("-" * 60)

def main():
    # Create search engine instance
    engine = MiniSearchEngine()

    # Load documents - make sure the path is correct
    if not engine.load_documents("cran.all.1400"):
        return

    # Interactive search interface
    print("\nMini Search Engine - Cranfield Collection")
    print("Type 'exit' to quit\n")

    for _ in range(3):
        try:
            query = input("Enter your search query: ").strip()
            if query.lower() == 'exit':
                break
            if not query:
                print("Please enter a query.")
                continue

            results = engine.search(query)
            engine.display_results(results)
        except KeyboardInterrupt:
            print("\nExiting...")
            break
        except Exception as e:
            print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()

def evaluate_search(results, relevant_docs, k=10):
    """
    حساب precision, recall, f1, accuracy لمجموعة نتائج البحث.
    - results: قائمة [(doc_id, score), ...] مرتبة حسب الأفضلية.
    - relevant_docs: مجموعة (set) من doc_id للوثائق الصحيحة لهذا الاستعلام.
    - k: عدد النتائج التي سيتم تقييمها (top-k).
    """
    # فقط أعلى k نتائج
    retrieved_docs = set([doc_id for doc_id, _ in results[:k]])
    relevant_docs = set(relevant_docs)

    true_positives = len(retrieved_docs & relevant_docs)
    false_positives = len(retrieved_docs - relevant_docs)
    false_negatives = len(relevant_docs - retrieved_docs)
    # في محركات البحث، true_negatives عادة لا تُحسب لأنها ضخمة جداً وغير مفيدة

    # Precision: نسبة النتائج المسترجعة التي هي فعلاً صحيحة
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    # Recall: نسبة النتائج الصحيحة التي تم استرجاعها
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    # F1-score
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    # Accuracy: (ليس شائعًا في محركات البحث، لكنه يحسب هكذا)
    # accuracy = (TP + TN) / (TP + FP + FN + TN)
    # سنهمل TN هنا، ونعتبر فقط الدقة على النتائج المسترجعة
    accuracy = true_positives / k if k > 0 else 0.0

    return precision, recall, f1, accuracy

query = "what is the basic mechanism of the transonic aileron buzz ."
results = MiniSearchEngine.search(query, top_k=10)
# ضع هنا قائمة الوثائق الصحيحة لهذا الاستعلام
relevant_docs = {"573", "329", "486"}  # عدل حسب الاستعلام

precision, recall, f1, accuracy = evaluate_search(results, relevant_docs, k=10)

print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-score: {f1:.3f}")
print(f"Accuracy: {accuracy:.3f}")

